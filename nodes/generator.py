from typing import Dict, Any, List
import sys
import os

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“ˆ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from utils.openai_client import OpenAIClient
from prompts.c1_prompt import C1_PROMPT_TEMPLATE
from prompts.c2_prompt import C2_PROMPT_TEMPLATE
from prompts.c3_prompt import C3_PROMPT_TEMPLATE
from prompts.c4_prompt import C4_PROMPT_TEMPLATE

def format_system_analysis_info(task_type: str, task_confidence: float, task_reason: str,
                               final_best_round: int, final_best_score: float, 
                               final_best_top_k: int, cocktails_count: int,
                               evaluation_scores: Dict[str, float], 
                               reflection_feedback: str) -> str:
    """
    format system analysis info for user-friendly
    
    Args:
        task_type: ë¶„ë¥˜ëœ íƒœìŠ¤í¬ íƒ€ì…
        task_confidence: ë¶„ë¥˜ ì‹ ë¢°ë„
        task_reason: ë¶„ë¥˜ ì´ìœ 
        final_best_round: ì„ íƒëœ ë¼ìš´ë“œ
        final_best_score: ìµœê³  ì ìˆ˜
        final_best_top_k: ì„ íƒëœ ë•Œì˜ Top-K
        cocktails_count: ì¶”ì²œëœ ì¹µí…Œì¼ ìˆ˜
        evaluation_scores: í‰ê°€ ì ìˆ˜ë“¤
        reflection_feedback: ë¦¬í”Œë ‰ì…˜ í”¼ë“œë°±
        
    Returns:
        í¬ë§·ëœ ì‹œìŠ¤í…œ ë¶„ì„ ì •ë³´ í…ìŠ¤íŠ¸
    """
    # íƒœìŠ¤í¬ íƒ€ì…ë³„ ì´ë¦„ ë§¤í•‘
    task_names = {
        "C1": "Color-Ingredient Visual Search",
        "C2": "Glass Type + Ingredient Matching", 
        "C3": "Multi-hop Ingredient Expansion Search",
        "C4": "Cocktail Recipe Similarity and Alternative Recommendation"
    }
    
    task_name = task_names.get(task_type, task_type)
    
    # classification info
    classification_info = f"""ğŸ¯ task classification:
- classification result: {task_type} ({task_name})
- confidence: {task_confidence:.1f}%
- classification reason: {task_reason}"""
    
    # selection info - round-wise situation description
    if final_best_round == 1:
        selection_reason = "ì²« ë²ˆì§¸ ê²€ìƒ‰ì—ì„œ ì¶©ë¶„í•œ í’ˆì§ˆ ë‹¬ì„±"
    elif final_best_score >= 80:
        selection_reason = f"Round {final_best_round}ì—ì„œ í’ˆì§ˆ ê¸°ì¤€(80ì ) ë‹¬ì„±"
    else:
        selection_reason = f"3íšŒ ë°˜ë³µ ì¤‘ Round {final_best_round}ì—ì„œ ìµœê³  í’ˆì§ˆ ë‹¬ì„±"
    
    selection_info = f"""ğŸ† final selection:
- selected round: Round {final_best_round}
- recommended cocktails: {cocktails_count} (Top-{final_best_top_k} search)
- quality score: {final_best_score:.1f}/100 points
- selection reason: {selection_reason}"""
    
    # í‰ê°€ ì ìˆ˜ - ë” ìƒì„¸í•œ ì„¤ëª…
    if evaluation_scores:
        try:
            relevance = float(evaluation_scores.get('relevance', 0))
            diversity = float(evaluation_scores.get('diversity', 0))
            completeness = float(evaluation_scores.get('completeness', 0))
            coherence = float(evaluation_scores.get('coherence', 0))
            
            evaluation_info = f"""ğŸ“Š evaluation details:
- relevance (Relevance): {relevance:.1f}/100 points - relevance to the question
- diversity (Diversity): {diversity:.1f}/100 points - recommendation diversity
- completeness (Completeness): {completeness:.1f}/100 points - requirement satisfaction
- coherence (Coherence): {coherence:.1f}/100 points - logical consistency
- overall score: {final_best_score:.1f}/100 points

ğŸ’¡ system feedback: {reflection_feedback}"""
        except (ValueError, TypeError):
            evaluation_info = f"""ğŸ“Š evaluation:
- overall score: {final_best_score:.1f}/100 points
- feedback: {reflection_feedback}"""
    else:
        evaluation_info = f"""ğŸ“Š evaluation:
- overall score: {final_best_score:.1f}/100 points"""
    
    return f"""
{classification_info}

{selection_info}

{evaluation_info}
"""

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = OpenAIClient()

def format_cocktails_for_response(cocktails: List[Dict[str, Any]]) -> str:
    """
    format cocktails list for final response
    
    Args:
        cocktails: cocktails info list
        
    Returns:
        í¬ë§·ëœ í…ìŠ¤íŠ¸
    """
    if not cocktails:
        return "No cocktails found."
    
    formatted_lines = []
    
    for i, cocktail in enumerate(cocktails, 1):
        name = cocktail.get('name', 'Unknown')
        category = cocktail.get('category', 'N/A')
        glass_type = cocktail.get('glassType', 'N/A')
        alcoholic = cocktail.get('alcoholic', 'N/A')
        description = cocktail.get('description', '')
        instructions = cocktail.get('instructions', '')
        
        # header
        formatted_lines.append(f"{i}. **{name}**")
        formatted_lines.append(f"   - category: {category}")
        formatted_lines.append(f"   - glass_type: {glass_type}")
        formatted_lines.append(f"   - alcoholic: {alcoholic}")
        
        # ingredients info
        recipe_ingredients = cocktail.get('recipe_ingredients', [])
        ingredients = cocktail.get('ingredients', [])
        
        if recipe_ingredients:
            formatted_lines.append("   - ingredients:")
            for ingredient_info in recipe_ingredients:
                measure = ingredient_info.get('measure', 'unknown')
                ingredient = ingredient_info.get('ingredient', 'unknown')
                formatted_lines.append(f"     â€¢ {measure} {ingredient}")
        elif ingredients:
            formatted_lines.append(f"   - ingredients: {', '.join(ingredients)}")
        
        # instructions
        if instructions:
            formatted_lines.append(f"   - instructions: {instructions}")
            
        # description
        if description:
            formatted_lines.append(f"   - description: {description}")
        
        formatted_lines.append("")  # empty line
    
    return "\n".join(formatted_lines)

def generate_final_response(user_query: str, cocktails: List[Dict[str, Any]], 
                          task_type: str, evaluation_scores: Dict[str, float],
                          reflection_feedback: str, task_confidence: float = 0,
                          task_reason: str = "", final_best_round: int = 1,
                          final_best_score: float = 0, final_best_top_k: int = 3) -> str:
    """
    ìµœì¢… ì‘ë‹µ ìƒì„±
    
    Args:
        user_query: ì‚¬ìš©ì ì§ˆë¬¸
        cocktails: ì¶”ì²œ ì¹µí…Œì¼ ë¦¬ìŠ¤íŠ¸
        task_type: íƒœìŠ¤í¬ íƒ€ì… (C1-C4)
        evaluation_scores: í‰ê°€ ì ìˆ˜ë“¤
        reflection_feedback: ë¦¬í”Œë ‰ì…˜ í”¼ë“œë°±
        
    Returns:
        ìµœì¢… ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    try:
        # íƒœìŠ¤í¬ë³„ í”„ë¡¬í”„íŠ¸ ì§ì ‘ ì‚¬ìš© (ì‘ë‹µ ìƒì„±ìš©)
        task_prompts = {
            "C1": C1_PROMPT_TEMPLATE,
            "C2": C2_PROMPT_TEMPLATE,
            "C3": C3_PROMPT_TEMPLATE,
            "C4": C4_PROMPT_TEMPLATE
        }
        task_prompt = task_prompts.get(task_type, C1_PROMPT_TEMPLATE)
        
        # ë””ë²„ê¹…: íƒœìŠ¤í¬ í”„ë¡¬í”„íŠ¸ í™•ì¸
        print(f"ğŸ” íƒœìŠ¤í¬ í”„ë¡¬í”„íŠ¸ í™•ì¸ ({task_type}):")
        print(f"   - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(task_prompt)} ê¸€ì")
        print(f"   - context í”Œë ˆì´ìŠ¤í™€ë” í¬í•¨: {'context' in task_prompt}")
        print(f"   - question í”Œë ˆì´ìŠ¤í™€ë” í¬í•¨: {'question' in task_prompt}")
        if 'context' in task_prompt:
            # contextê°€ ì–´ë””ì— ìœ„ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            context_pos = task_prompt.find('{context}')
            context_preview = task_prompt[max(0, context_pos-50):context_pos+100] if context_pos != -1 else "NOT_FOUND"
            print(f"   - context ìœ„ì¹˜ ì£¼ë³€: ...{context_preview}")
        
        # ì¹µí…Œì¼ ì •ë³´ í¬ë§·íŒ…
        cocktails_context = format_cocktails_for_response(cocktails)
        
        # ë””ë²„ê¹…: ì¹µí…Œì¼ ì»¨í…ìŠ¤íŠ¸ ë‚´ìš© í™•ì¸
        print(f"ğŸ” ì¹µí…Œì¼ ì»¨í…ìŠ¤íŠ¸ í™•ì¸:")
        print(f"   - ì¹µí…Œì¼ ìˆ˜: {len(cocktails)}")
        if cocktails:
            cocktail_names = [c.get('name', 'Unknown') for c in cocktails]
            print(f"   - ì¹µí…Œì¼ ì´ë¦„ë“¤: {cocktail_names}")
        print(f"   - í¬ë§·ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(cocktails_context)} ê¸€ì")
        print(f"   - ì»¨í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 200ì): {cocktails_context}")
        
        # í‰ê°€ ì •ë³´ í¬ë§·íŒ…
        evaluation_text = ""
        if evaluation_scores:
            # ê° ì ìˆ˜ë¥¼ floatë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ê³„ì‚°
            try:
                relevance = float(evaluation_scores.get('relevance', 0))
                diversity = float(evaluation_scores.get('diversity', 0))
                completeness = float(evaluation_scores.get('completeness', 0))
                coherence = float(evaluation_scores.get('coherence', 0))
                
                overall_score = (relevance + diversity + completeness + coherence) / 4
                
                evaluation_text = f"""
evaluation scores:
- relevance: {relevance:.1f} points
- diversity: {diversity:.1f} points  
- completeness: {completeness:.1f} points
- coherence: {coherence:.1f} points
- overall: {overall_score:.1f} points

{reflection_feedback}
"""
            except (ValueError, TypeError) as e:
                print(f"âš ï¸ í‰ê°€ ì ìˆ˜ ë³€í™˜ ì˜¤ë¥˜: {e}")
                evaluation_text = f"""
Evaluation scores conversion error occurred.
original data: {evaluation_scores}

{reflection_feedback}
"""
        
        # í”„ë¡¬í”„íŠ¸ì— ì •ë³´ ì‚½ì…
        prompt = task_prompt.format(
            question=user_query,
            context=cocktails_context
        )
        
        # ì‹œìŠ¤í…œ ë¶„ì„ ì •ë³´ ìƒì„±
        system_analysis = format_system_analysis_info(
            task_type=task_type,
            task_confidence=task_confidence,
            task_reason=task_reason,
            final_best_round=final_best_round,
            final_best_score=final_best_score,
            final_best_top_k=final_best_top_k,
            cocktails_count=len(cocktails),
            evaluation_scores=evaluation_scores,
            reflection_feedback=reflection_feedback
        )

        # ìˆœìˆ˜í•œ ì¹µí…Œì¼ ì •ë³´ë§Œìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì‹œìŠ¤í…œ ë¶„ì„ ì •ë³´ ì œì™¸)
        enhanced_prompt = f"""{prompt}

Based on the cocktail information above, please recommend cocktails with detailed explanations that are helpful for the user.
Include reasons for recommendation, flavor characteristics, and situational suggestions in your explanation."""
        print(f"ğŸ¯ ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘... ({task_type})")
        
        # LLMì´ ë°›ëŠ” ìµœì¢… ì»¨í…ìŠ¤íŠ¸ë¥¼ HTMLë¡œ í‘œì‹œ
        from IPython.display import display, HTML
        
        # HTML í˜•íƒœë¡œ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        html_content = f"""
        <div style="border: 2px solid #4CAF50; border-radius: 10px; padding: 20px; margin: 10px 0; background-color: #f8f9fa;">
            <h3 style="color: #2E8B57; margin-top: 0;">ğŸ¤– LLMì´ ë°›ëŠ” ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ({task_type})</h3>
            <div style="background-color: white; border: 1px solid #ddd; border-radius: 5px; padding: 15px; font-family: monospace; white-space: pre-wrap; max-height: 600px; overflow-y: auto;">
{enhanced_prompt}
            </div>
        </div>
        """
        
        display(HTML(html_content))
        
        # OpenAI API í˜¸ì¶œ
        response = openai_client.generate(enhanced_prompt, max_tokens=1500)
        
        return response
        
    except Exception as e:
        print(f"âŒ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
        # ê¸°ë³¸ ì‘ë‹µ ìƒì„±
        cocktails_text = format_cocktails_for_response(cocktails)
        return f"""Sorry, an error occurred while generating the response.

We recommend the following cocktails:

{cocktails_text}

ì˜¤ë¥˜ ë‚´ìš©: {str(e)}"""

def generator(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ë¹„êµìš© ì‘ë‹µì„ ìƒì„±í•˜ëŠ” Generator ë…¸ë“œ
    
    ì´ˆê¸° ë‹µë³€ê³¼ ìµœì¢… ë‹µë³€ì„ ê°ê° ìƒì„±í•˜ì—¬ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    
    Args:
        state: íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ì´ˆê¸° ë‹µë³€ê³¼ ìµœì¢… ë‹µë³€ì´ í¬í•¨ëœ ìƒíƒœ ë”•ì…”ë„ˆë¦¬
    """
    # í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
    user_query = state.get("input_text_with_image", state.get("input_text", ""))
    task_type = state.get("task_type", "C1")
    iteration_count = state.get("iteration_count", 0)
    
    print(f"\nğŸ“ ë¹„êµìš© ì‘ë‹µ ìƒì„± ì‹œì‘ ({task_type}, {iteration_count}íšŒ ë°˜ë³µ)")
    
    # 1. ì´ˆê¸° ë‹µë³€ ìƒì„± (Round 1 ê²°ê³¼ ì‚¬ìš©)
    initial_results = state.get("initial_search_results", [])
    if initial_results:
        print(f"ğŸ¥‰ ì´ˆê¸° ë‹µë³€ ìƒì„± ì¤‘... ({len(initial_results)}ê°œ ì¹µí…Œì¼)")
        try:
            # íƒœìŠ¤í¬ ë¶„ë¥˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            task_confidence = state.get("task_confidence", 0)
            task_reason = state.get("task_reason", "ì •ë³´ ì—†ìŒ")
            
            # ì´ˆê¸° í‰ê°€ ì ìˆ˜ ì‚¬ìš© (reflectionì—ì„œ ì €ì¥í•œ ê²ƒ)
            initial_evaluation_scores = state.get("initial_evaluation_scores", {})
            initial_score = state.get("initial_score", 0)
            initial_feedback = state.get("initial_feedback", "ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼")
            
            # ì´ˆê¸° ì ìˆ˜ê°€ ì—†ëŠ” ê²½ìš°, best_resultì—ì„œ Round 1 ì •ë³´ ì°¾ê¸°
            if initial_score == 0 and "best_result" in state:
                best_result = state["best_result"]
                if best_result.get("iteration") == 1:
                    # best_resultê°€ Round 1 ê²°ê³¼ì¸ ê²½ìš°
                    initial_score = best_result.get("score", 0)
                    initial_evaluation_scores = best_result.get("evaluation", {})
                    initial_feedback = initial_evaluation_scores.get("feedback", "Round 1 ê²€ìƒ‰ ê²°ê³¼")
                    print(f"ğŸ“‹ best_resultì—ì„œ Round 1 ì ìˆ˜ ë³µêµ¬: {initial_score}ì ")
                else:
                    # ë””ë²„ê¹…ì„ ìœ„í•´ debug_infoì—ì„œ reflection íˆìŠ¤í† ë¦¬ í™•ì¸
                    reflection_history = state.get("debug_info", {}).get("reflection_history", [])
                    if reflection_history:
                        round1_result = reflection_history[0]  # ì²« ë²ˆì§¸ ë¼ìš´ë“œ
                        initial_score = round1_result.get("score", 0)
                        initial_evaluation_scores = round1_result.get("scores", {})
                        initial_feedback = initial_evaluation_scores.get("feedback", "Round 1 ê²€ìƒ‰ ê²°ê³¼")
                        print(f"ğŸ“‹ reflection_historyì—ì„œ Round 1 ì ìˆ˜ ë³µêµ¬: {initial_score}ì ")
            
            print(f"ğŸ” ì´ˆê¸° í‰ê°€ ì ìˆ˜ (ìµœì¢… í™•ì¸):")
            print(f"   - initial_evaluation_scores: {initial_evaluation_scores}")
            print(f"   - initial_score: {initial_score}")
            print(f"   - initial_feedback: {initial_feedback}")
            
            initial_response = generate_final_response(
                user_query=user_query,
                cocktails=initial_results,
                task_type=task_type,
                evaluation_scores=initial_evaluation_scores,
                reflection_feedback=initial_feedback,
                task_confidence=task_confidence,
                task_reason=task_reason,
                final_best_round=1,
                final_best_score=initial_score,
                final_best_top_k=len(initial_results)
            )
            state["initial_response"] = initial_response
            print(f"âœ… ì´ˆê¸° ë‹µë³€ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì´ˆê¸° ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            state["initial_response"] = f"ì´ˆê¸° ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    # 2. ìµœì¢… ë‹µë³€ ìƒì„± (ìµœê³  ì ìˆ˜ ë¼ìš´ë“œ ê²°ê³¼ ì‚¬ìš©)
    final_results = state.get("final_search_results", [])
    if final_results:
        final_best_round = state.get("final_best_round", 3)
        final_best_score = state.get("final_best_score", 0)
        final_best_top_k = state.get("final_best_top_k", len(final_results))
        
        print(f"ğŸ† ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘... (Round {final_best_round}, {len(final_results)}ê°œ ì¹µí…Œì¼, {final_best_score:.1f}ì )")
        
        evaluation_scores = state.get("evaluation_scores", {})
        reflection_feedback = state.get("reflection_feedback", "")
        
        try:
            # íƒœìŠ¤í¬ ë¶„ë¥˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            task_confidence = state.get("task_confidence", 0)
            task_reason = state.get("task_reason", "ì •ë³´ ì—†ìŒ")
            
            final_response = generate_final_response(
                user_query=user_query,
                cocktails=final_results,
                task_type=task_type,
                evaluation_scores=evaluation_scores,
                reflection_feedback=reflection_feedback,
                task_confidence=task_confidence,
                task_reason=task_reason,
                final_best_round=final_best_round,
                final_best_score=final_best_score,
                final_best_top_k=final_best_top_k
            )
            state["final_response"] = final_response
            state["final_text"] = final_response  # í•˜ìœ„ í˜¸í™˜ì„±
            state["final_cocktails"] = final_results  # í•˜ìœ„ í˜¸í™˜ì„±
            print(f"âœ… ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ìµœì¢… ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            state["final_response"] = f"ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            state["final_text"] = f"ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            state["final_cocktails"] = final_results
    
    # 3. ì‘ë‹µì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì²˜ë¦¬
    if not initial_results and not final_results:
        print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.")
        error_response = f"""ì£„ì†¡í•©ë‹ˆë‹¤. "{user_query}"ì— ëŒ€í•œ ì ì ˆí•œ ì¹µí…Œì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

ë‹¤ì‹œ í•œë²ˆ ë‹¤ë¥¸ í‚¤ì›Œë“œë‚˜ ì„¤ëª…ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”:
- ì„ í˜¸í•˜ëŠ” ìƒ‰ìƒì´ë‚˜ ë§›
- íŠ¹ì • ì¬ë£Œë‚˜ ë² ì´ìŠ¤ ìˆ 
- ë§ˆì‹œê³  ì‹¶ì€ ìƒí™©ì´ë‚˜ ë¶„ìœ„ê¸°
- ê¸€ë¼ìŠ¤ íƒ€ì…ì´ë‚˜ ìŠ¤íƒ€ì¼"""
        
        state["initial_response"] = error_response
        state["final_response"] = error_response
        state["final_text"] = error_response
        state["final_cocktails"] = []
    
    # ë””ë²„ê·¸ ì •ë³´ ì¶”ê°€
    if "debug_info" not in state:
        state["debug_info"] = {}
    
    state["debug_info"]["generation"] = {
        "task_type": task_type,
        "initial_cocktails_count": len(initial_results),
        "final_cocktails_count": len(final_results),
        "initial_response_length": len(state.get("initial_response", "")),
        "final_response_length": len(state.get("final_response", "")),
        "iteration_count": iteration_count,
        "final_best_round": state.get("final_best_round", 0),
        "final_best_score": state.get("final_best_score", 0),
        "final_best_top_k": state.get("final_best_top_k", 0)
    }
    
    return state