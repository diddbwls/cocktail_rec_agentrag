from typing import Dict, Any, List
import sys
import os

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“ˆ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from utils.openai_client import OpenAIClient
from prompts.reflection_prompt import REFLECTION_PROMPT_TEMPLATE

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = OpenAIClient()

def format_search_results(results: List[Dict[str, Any]]) -> str:
    """
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ í‰ê°€ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
    
    Args:
        results: ê²€ìƒ‰ëœ ì¹µí…Œì¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        í¬ë§·ëœ í…ìŠ¤íŠ¸
    """
    if not results:
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    formatted_lines = []
    for i, cocktail in enumerate(results, 1):
        name = cocktail.get('name', 'Unknown')
        category = cocktail.get('category', 'N/A')
        glass_type = cocktail.get('glassType', 'N/A')
        alcoholic = cocktail.get('alcoholic', 'N/A')
        description = cocktail.get('description', 'N/A')
        
        # ì¬ë£Œ ì •ë³´
        ingredients = cocktail.get('ingredients', [])
        recipe_ingredients = cocktail.get('recipe_ingredients', [])
        
        if recipe_ingredients:
            ingredients_text = ", ".join([f"{item.get('measure', '')} {item.get('ingredient', '')}" 
                                        for item in recipe_ingredients])
        else:
            ingredients_text = ", ".join(ingredients)
        
        formatted_lines.append(f"{i}. {name}")
        formatted_lines.append(f"   ì¹´í…Œê³ ë¦¬: {category} | ê¸€ë¼ìŠ¤: {glass_type} | ì•Œì½”ì˜¬: {alcoholic}")
        formatted_lines.append(f"   ì¬ë£Œ: {ingredients_text}")
        formatted_lines.append(f"   ì„¤ëª…: {description}")
        formatted_lines.append("")  # ë¹ˆ ì¤„
    
    return "\n".join(formatted_lines)

def evaluate_search_quality(user_query: str, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ 4ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€
    
    Args:
        user_query: ì‚¬ìš©ì ì§ˆë¬¸
        search_results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    try:
        # ê¸°ì¡´ reflection í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        formatted_results = format_search_results(search_results)
        
        # í”„ë¡¬í”„íŠ¸ì— ë°ì´í„° ì‚½ì…
        prompt = REFLECTION_PROMPT_TEMPLATE.format(
            user_query=user_query,
            num_results=len(search_results),
            search_results=formatted_results
        )
        
        print(f"ğŸ¤” ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ ì¤‘...")
        
        # OpenAI API í˜¸ì¶œ
        response = openai_client.generate(prompt, response_format="json")
        
        # JSON íŒŒì‹±
        evaluation = openai_client.parse_json_response(response)
        
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ê²€ì¦
        if "error" in evaluation:
            print(f"âš ï¸ í‰ê°€ íŒŒì‹± ì˜¤ë¥˜, ê¸°ë³¸ê°’ ì‚¬ìš©")
            evaluation = {
                "relevance": 50.0,
                "diversity": 50.0,
                "completeness": 50.0,
                "coherence": 50.0,
                "overall_score": 50.0,
                "feedback": "í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "suggestions": ["ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."],
                "should_retry": True
            }
        
        # ì ìˆ˜ ê²€ì¦ ë° ë³€í™˜
        scores = {}
        for key in ["relevance", "diversity", "completeness", "coherence"]:
            try:
                scores[key] = float(evaluation.get(key, 50))
                # 0-100 ë²”ìœ„ í™•ì¸
                scores[key] = max(0, min(100, scores[key]))
            except (ValueError, TypeError):
                scores[key] = 50.0
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = sum(scores.values()) / len(scores)
        
        # ì¬ì‹œë„ ì—¬ë¶€ ê²°ì • (80ì  ë¯¸ë§Œ ì‹œ ì¬ì‹œë„)
        should_retry = overall_score < 80
        
        result = {
            "relevance": scores["relevance"],
            "diversity": scores["diversity"], 
            "completeness": scores["completeness"],
            "coherence": scores["coherence"],
            "overall_score": overall_score,
            "feedback": evaluation.get("feedback", "í‰ê°€ ì™„ë£Œ"),
            "suggestions": evaluation.get("suggestions", []),
            "should_retry": should_retry
        }
        
        print(f"ğŸ“Š í‰ê°€ ì™„ë£Œ: {overall_score:.1f}ì  (ì¬ì‹œë„: {'ì˜ˆ' if should_retry else 'ì•„ë‹ˆì˜¤'})")
        
        return result
        
    except Exception as e:
        print(f"âŒ í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {e}")
        return {
            "relevance": 50.0,
            "diversity": 50.0,
            "completeness": 50.0,
            "coherence": 50.0,
            "overall_score": 50.0,
            "feedback": f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "suggestions": ["ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¬ì‹œë„ê°€ í•„ìš”í•©ë‹ˆë‹¤."],
            "should_retry": True
        }

def reflection(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê³  í•„ìš”ì‹œ ì¬ê²€ìƒ‰ì„ ê²°ì •í•˜ëŠ” ë¦¬í”Œë ‰ì…˜ ë…¸ë“œ
    
    ìµœëŒ€ 3íšŒê¹Œì§€ ë°˜ë³µí•˜ë©°, ê° ë¼ìš´ë“œë§ˆë‹¤ top-kë¥¼ 1ì”© ì¦ê°€ì‹œí‚´
    
    Args:
        state: íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë”•ì…”ë„ˆë¦¬
        
    Returns:
        í‰ê°€ ê²°ê³¼ì™€ ì¬ì‹œë„ ì—¬ë¶€ê°€ í¬í•¨ëœ ìƒíƒœ ë”•ì…”ë„ˆë¦¬
    """
    # í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
    user_query = state.get("input_text_with_image", state.get("input_text", ""))
    search_results = state.get("search_results", [])
    iteration_count = state.get("iteration_count", 0)
    current_top_k = state.get("current_top_k", 3)
    
    print(f"\nğŸ”„ Reflection (ë¼ìš´ë“œ {iteration_count + 1}/3, Top-K: {current_top_k})")
    
    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
    if not search_results:
        print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ì„œ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        state["score"] = 0.0
        state["evaluation_scores"] = {
            "relevance": 0.0,
            "diversity": 0.0,
            "completeness": 0.0,
            "coherence": 0.0
        }
        state["reflection_feedback"] = "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        return state
    
    # í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
    evaluation = evaluate_search_quality(user_query, search_results)
    
    # í‰ê°€ ê²°ê³¼ ì €ì¥
    state["score"] = evaluation["overall_score"]
    state["evaluation_scores"] = {
        "relevance": evaluation["relevance"],
        "diversity": evaluation["diversity"],
        "completeness": evaluation["completeness"],
        "coherence": evaluation["coherence"]
    }
    state["reflection_feedback"] = evaluation["feedback"]
    
    # ìµœê³  ì ìˆ˜ ê²°ê³¼ ì—…ë°ì´íŠ¸ (ë™ì  ì‹œ ë” ë†’ì€ ë¼ìš´ë“œ ì„ íƒ)
    current_score = evaluation["overall_score"]
    best_score = state.get("best_result", {}).get("score", 0)
    
    should_update = False
    update_reason = ""
    
    if "best_result" not in state:
        should_update = True
        update_reason = "ì²« ë²ˆì§¸ ê²°ê³¼"
    elif current_score > best_score:
        should_update = True
        update_reason = f"ë” ë†’ì€ ì ìˆ˜ ({current_score:.1f} > {best_score:.1f})"
    elif current_score == best_score:
        # ë™ì ì¼ ë•ŒëŠ” ë” ë†’ì€ ë¼ìš´ë“œ(ë” ë§ì€ ì¹µí…Œì¼) ì„ íƒ
        should_update = True
        update_reason = f"ë™ì ì´ë¯€ë¡œ ë‹¤ì–‘ì„± ì¦ê°€ (Round {iteration_count + 1}, Top-{current_top_k})"
    
    if should_update:
        state["best_result"] = {
            "score": evaluation["overall_score"],
            "results": search_results.copy(),
            "evaluation": evaluation,
            "top_k": current_top_k,
            "iteration": iteration_count + 1
        }
        print(f"ğŸ† ìµœê³  ê²°ê³¼ ì—…ë°ì´íŠ¸: {evaluation['overall_score']:.1f}ì  ({update_reason})")
    
    # ë°˜ë³µ íšŸìˆ˜ ì¦ê°€
    state["iteration_count"] = iteration_count + 1
    
    # ì´ˆê¸° ê²°ê³¼ ì €ì¥ (Round 1 ì™„ë£Œ ì‹œ)
    if state["iteration_count"] == 1:
        print(f"ğŸ’¾ ì´ˆê¸° ê²°ê³¼ ì €ì¥ (Round 1, {len(search_results)}ê°œ ì¹µí…Œì¼)")
        state["initial_search_results"] = search_results.copy()
        # ì´ˆê¸° í‰ê°€ ì •ë³´ë„ ì €ì¥
        state["initial_evaluation_scores"] = {
            "relevance": evaluation["relevance"],
            "diversity": evaluation["diversity"],
            "completeness": evaluation["completeness"],
            "coherence": evaluation["coherence"]
        }
        state["initial_score"] = evaluation["overall_score"]
        state["initial_feedback"] = evaluation["feedback"]
        
        # ë””ë²„ê·¸: ì €ì¥ë˜ëŠ” ì´ˆê¸° í‰ê°€ ì ìˆ˜ í™•ì¸
        print(f"ğŸ” ì´ˆê¸° í‰ê°€ ì ìˆ˜ ì €ì¥ ë””ë²„ê·¸:")
        print(f"   - evaluation (ì›ë³¸): {evaluation}")
        print(f"   - initial_evaluation_scores (ì €ì¥): {state['initial_evaluation_scores']}")
        print(f"   - initial_score (ì €ì¥): {state['initial_score']}")
        # ì´ˆê¸° ë‹µë³€ì€ Generatorì—ì„œ ìƒì„±ë¨
    
    # ì¬ì‹œë„ ê²°ì • ë¡œì§
    should_retry = False
    
    # 1. ì ìˆ˜ê°€ 80ì  ì´ìƒì´ë©´ ì¦‰ì‹œ ì™„ë£Œ
    if evaluation["overall_score"] >= 80:
        print(f"âœ… í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡± ({evaluation['overall_score']:.1f}ì  >= 80ì )")
        should_retry = False
        
        # í˜„ì¬ ê²°ê³¼ë¥¼ ìµœì¢… ê²°ê³¼ë¡œ ì €ì¥ (80ì  ì´ìƒ ë‹¬ì„±)
        current_round = state["iteration_count"]
        print(f"ğŸ† í’ˆì§ˆ ê¸°ì¤€ ë‹¬ì„±: Round {current_round} ({evaluation['overall_score']:.1f}ì , Top-K: {current_top_k})")
        print(f"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ (Round {current_round}, {len(search_results)}ê°œ ì¹µí…Œì¼)")
        
        state["final_search_results"] = search_results.copy()
        state["final_best_score"] = evaluation['overall_score']
        state["final_best_round"] = current_round
        state["final_best_top_k"] = current_top_k
        state["reflection_feedback"] = f"í’ˆì§ˆ ê¸°ì¤€ ë‹¬ì„±: {evaluation['overall_score']:.1f}ì  (Round {current_round})"
    # 2. ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬ ì‹œ ì¢…ë£Œ
    elif state["iteration_count"] >= 3:
        print(f"â¸ï¸ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬ (3íšŒ), ìµœê³  ì ìˆ˜ ê²°ê³¼ ì„ íƒ")
        should_retry = False
        
        # ìµœê³  ì ìˆ˜ ê²°ê³¼ë¥¼ ìµœì¢… ê²°ê³¼ë¡œ ì‚¬ìš©
        if "best_result" in state:
            best_result = state["best_result"]
            final_score = best_result["score"]
            best_round = best_result["iteration"]
            best_top_k = best_result["top_k"]
            
            print(f"ğŸ† ìµœê³  ì ìˆ˜ ì„ íƒ: Round {best_round} ({final_score:.1f}ì , Top-K: {best_top_k})")
            print(f"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ (Round {best_round}, {len(best_result['results'])}ê°œ ì¹µí…Œì¼)")
            
            state["final_search_results"] = best_result["results"].copy()
            state["final_best_score"] = final_score
            state["final_best_round"] = best_round
            state["final_best_top_k"] = best_top_k
            
            # ìµœì¢… ë‹µë³€ì€ Generatorì—ì„œ ìƒì„±ë¨
            state["reflection_feedback"] = f"3íšŒ ë°˜ë³µ ì™„ë£Œ: ìµœê³  ì ìˆ˜ {final_score:.1f}ì  (Round {best_round})"
        else:
            # í´ë°±: best_resultê°€ ì—†ìœ¼ë©´ í˜„ì¬ ê²°ê³¼ ì‚¬ìš©
            print(f"âš ï¸ best_result ì—†ìŒ, í˜„ì¬ ë¼ìš´ë“œ ê²°ê³¼ ì‚¬ìš©")
            print(f"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ (Round 3, {len(search_results)}ê°œ ì¹µí…Œì¼)")
            state["final_search_results"] = search_results.copy()
            state["final_best_score"] = evaluation['overall_score']
            state["final_best_round"] = 3
            state["final_best_top_k"] = current_top_k
            state["reflection_feedback"] = f"3íšŒ ë°˜ë³µ ì™„ë£Œ: ìµœì¢… ì ìˆ˜ {evaluation['overall_score']:.1f}ì "
    # 3. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì¬ì‹œë„
    else:
        should_retry = True
        # top-k ì¦ê°€
        state["current_top_k"] = current_top_k + 1
        print(f"ğŸ”„ ì¬ì‹œë„ ê²°ì •: Top-Kë¥¼ {current_top_k + 1}ë¡œ ì¦ê°€")
        print(f"ğŸ“‹ í”¼ë“œë°±: {evaluation['feedback']}")
        if evaluation['suggestions']:
            print(f"ğŸ’¡ ê°œì„  ì œì•ˆ: {', '.join(evaluation['suggestions'])}")
    
    # ë””ë²„ê·¸ ì •ë³´ ì—…ë°ì´íŠ¸
    if "debug_info" not in state:
        state["debug_info"] = {}
    
    if "reflection_history" not in state["debug_info"]:
        state["debug_info"]["reflection_history"] = []
    
    state["debug_info"]["reflection_history"].append({
        "iteration": iteration_count + 1,
        "top_k": current_top_k,
        "score": evaluation["overall_score"],
        "scores": evaluation,
        "should_retry": should_retry,
        "results_count": len(search_results)
    })
    
    # should_retry ìƒíƒœëŠ” pipelineì˜ ì¡°ê±´ë¶€ ì—£ì§€ì—ì„œ ì‚¬ìš©ë¨
    state["should_retry"] = should_retry
    
    # ìºì‹œ ë³´ì¡´ í™•ì¸ (ë””ë²„ê¹…)
    if "full_ranked_cocktails" in state:
        print(f"ğŸ” Reflection ì¢…ë£Œ ì‹œ ìºì‹œ ìƒíƒœ: {len(state.get('full_ranked_cocktails', []))}ê°œ")
    else:
        print(f"âš ï¸ Reflection ì¢…ë£Œ ì‹œ full_ranked_cocktails í‚¤ ì—†ìŒ")
    
    return state