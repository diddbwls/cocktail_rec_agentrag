# #!/usr/bin/env python3
# """
# gpt-4o-mini ëª¨ë¸ í‰ê°€ìš© ë‹µë³€ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (Ablation: Simple RAG)
# ì´ íŒŒì¼ì„ ì‹¤í–‰í•˜ë©´ output í´ë”ì— gpt-4o-miniëª¨ë¸ë¡œ cocktail_eval_dataì— ëŒ€í•´ ìƒì„±í•œ ë‹µë³€ íŒŒì¼ì´ ìƒì„±ë¨

# Ablation ì¡°ê±´: task ë¶„ë¥˜ì™€ reflectionì€ ì‚¬ìš©í•˜ì§€ë§Œ, ë³µì¡í•œ graph retrieval ëŒ€ì‹  ë‹¨ìˆœ ìœ ì‚¬ë„ ë¹„êµë§Œ ì‚¬ìš©
# """
# import pandas as pd
# import json
# import os
# import sys
# import time
# from pathlib import Path
# from tqdm import tqdm
# from typing import Dict, Any

# # ìƒìœ„ ë””ë ‰í† ë¦¬ ì¶”ê°€
# current_dir = Path(__file__).parent
# parent_dir = current_dir.parent
# sys.path.append(str(parent_dir))

# ####################################################
# # ëª¨ë¸ ì„¤ì • 
# import utils.config as config
# config.LLM_MODEL = "gpt-4o-mini"
# EVAL_DATA_COUNT = 5  # í‰ê°€í•  ë°ì´í„° ê°œìˆ˜ (ì „ì²´: None, ì¼ë¶€: ìˆ«ì)
# ####################################################

# from nodes.user_question import describe_image
# from nodes.task_classifier import query_classification
# from nodes.reflection import reflection
# from nodes.generator import generator
# from simple_retrieval import SimpleRetrieval

# # í‰ê°€ ë°ì´í„° path (ë¶€ëª¨ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
# eval_data_path = './data/cocktail_eval_data.csv'

# # ì§ˆë¬¸ ì´ë¯¸ì§€ path
# question_image_path = './data/image/'

# # ì¶œë ¥ íŒŒì¼ path
# output_file = f'./ablation_llm+rag/output/{config.LLM_MODEL}_ablation_simple_rag.csv'

# def load_eval_data():
#     """í‰ê°€ ë°ì´í„° ë¡œë“œ"""
#     df = pd.read_csv(eval_data_path)
#     if EVAL_DATA_COUNT is None:
#         return df  # ì „ì²´ ë°ì´í„°
#     else:
#         return df.iloc[0:EVAL_DATA_COUNT]  # ì§€ì •ëœ ê°œìˆ˜ë§Œ

# def run_simple_rag_pipeline(user_question: str) -> Dict[str, Any]:
#     """ë‹¨ìˆœ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ablation: ë³µì¡í•œ graph retrieval ëŒ€ì‹  ë‹¨ìˆœ ìœ ì‚¬ë„ ë¹„êµ)"""
#     state = {
#         "input_text_with_image": user_question,
#         "debug_info": {"reflection_history": []},
#         "current_top_k": config.FINAL_TOP_K,
#         "iteration_count": 0
#     }
    
#     # 1. Task Classification
#     print("ğŸ“Š Task Classification...")
#     state = query_classification(state)
    
#     retriever = SimpleRetrieval()
#     max_iterations = 3
#     best_result = None
    
#     # Iterative retrieval with reflection (like original, but with simple similarity)
#     for iteration in range(max_iterations):
#         current_top_k = state["current_top_k"]
#         print(f"\nğŸ”„ Round {iteration + 1}: Top-K = {current_top_k}")
        
#         # 2. Simple Retrieval (similarity-based)
#         print("ğŸ” Simple Retrieval...")
#         search_results = retriever.retrieve(user_question, top_k=current_top_k)
#         state["search_results"] = search_results
        
#         # Store initial results
#         if iteration == 0:
#             state["initial_search_results"] = search_results
        
#         # 3. Reflection
#         print("ğŸ¤” Reflection...")
#         state = reflection(state)
        
#         # Update best result if better
#         current_score = state.get("score", 0)
#         if best_result is None or current_score > best_result["score"]:
#             best_result = {
#                 "score": current_score,
#                 "results": search_results.copy(),
#                 "iteration": iteration + 1,
#                 "top_k": current_top_k
#             }
        
#         # Check if should continue
#         should_retry = state.get("should_retry", False)
#         if not should_retry or current_score >= 80:
#             print(f"âœ… Stopping: should_retry={should_retry}, score={current_score}")
#             break
            
#         # Increase top_k for next iteration
#         state["current_top_k"] = current_top_k + 1
#         state["iteration_count"] = iteration + 1
    
#     # Use best result
#     final_search_results = best_result["results"] if best_result else search_results
#     state["final_search_results"] = final_search_results
#     state["search_results"] = final_search_results  # For generator
    
#     # 4. Response Generation
#     print("ğŸ“¤ Response Generation...")
#     state = generator(state)
    
#     # Extract data for ablation
#     final_response = state.get("final_response", "")
#     final_context = retriever.format_cocktails_as_context(final_search_results)
    
#     # Get reflection scores from evaluation_scores
#     eval_scores = state.get("evaluation_scores", {})
#     reflection_result = {
#         "relevance": eval_scores.get("relevance", 0),
#         "diversity": eval_scores.get("diversity", 0),
#         "completeness": eval_scores.get("completeness", 0),
#         "coherence": eval_scores.get("coherence", 0),
#         "overall_score": state.get("score", 0),
#         "should_retry": state.get("should_retry", False)
#     }
    
#     # Compile final state  
#     final_state = {
#         "input_text_with_image": user_question,
#         "task_type": state.get("task_type", "Unknown"),
#         "task_confidence": state.get("task_confidence", 0.0),
#         "initial_search_results": state.get("initial_search_results", []),
#         "initial_response": state.get("initial_response", ""),
#         "final_cocktails": final_search_results,
#         "final_response": final_response,
#         "final_context": final_context,
#         "iteration_count": state.get("iteration_count", 0) + 1,
#         "reflection_result": reflection_result,
#         "debug_info": state.get("debug_info", {})
#     }
    
#     return final_state

# def extract_pipeline_data(state: Dict[str, Any]) -> Dict[str, Any]:
#     """íŒŒì´í”„ë¼ì¸ stateì—ì„œ í‰ê°€ ë°ì´í„° ì¶”ì¶œ"""
#     try:
#         # ê¸°ë³¸ ì •ë³´
#         user_question = state.get("input_text_with_image", "")
#         final_cocktails = state.get("final_cocktails", [])
        
#         # Round 1 ì •ë³´ (initial)
#         initial_search_results = state.get("initial_search_results", [])
#         round1_cocktails = [c.get('name', '') for c in initial_search_results]
        
#         # ë‹µë³€ ì •ë³´
#         initial_response = state.get("initial_response", "")
#         final_response = state.get("final_response", "")
        
#         # ì»¨í…ìŠ¤íŠ¸ ì •ë³´
#         round1_context = state.get("final_context", "")
#         final_context = state.get("final_context", "")
        
#         # reflection ê²°ê³¼
#         reflection_result = state.get("reflection_result", {})
        
#         # scores ë”•ì…”ë„ˆë¦¬ë¥¼ ê°œë³„ ì»¬ëŸ¼ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜
#         def extract_scores(scores_dict, prefix):
#             """scores ë”•ì…”ë„ˆë¦¬ë¥¼ ê°œë³„ ì»¬ëŸ¼ìœ¼ë¡œ ë¶„ë¦¬"""
#             return {
#                 f'{prefix}_relevance': scores_dict.get('relevance', 0),
#                 f'{prefix}_diversity': scores_dict.get('diversity', 0),
#                 f'{prefix}_completeness': scores_dict.get('completeness', 0),
#                 f'{prefix}_coherence': scores_dict.get('coherence', 0),
#                 f'{prefix}_overall_score': scores_dict.get('overall_score', 0),
#                 f'{prefix}_should_retry': scores_dict.get('should_retry', False)
#             }
        
#         # Round1ê³¼ Final scoresë¥¼ ê°œë³„ ì»¬ëŸ¼ìœ¼ë¡œ ë¶„ë¦¬
#         round1_score_columns = extract_scores(reflection_result, 'round1')
#         final_score_columns = extract_scores(reflection_result, 'final')
        
#         result = {
#             'user_question': user_question,
#             'task_type': state.get("task_type", "Unknown"),
#             'task_confidence': state.get("task_confidence", 0.0),
#             'round1_cocktails': round1_cocktails,
#             'round1_context': round1_context,
#             'round1_answer': initial_response,
#             'final_cocktails': [c.get('name', '') for c in final_cocktails],
#             'final_context': final_context,
#             'final_answer': final_response,
#             'iteration_count': state.get("iteration_count", 1),
#             'final_round': 1
#         }
        
#         # scores ê°œë³„ ì»¬ëŸ¼ë“¤ ì¶”ê°€
#         result.update(round1_score_columns)
#         result.update(final_score_columns)
        
#         return result
        
#     except Exception as e:
#         print(f"âš ï¸ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
#         error_result = {
#             'user_question': state.get("input_text_with_image", ""),
#             'task_type': "Unknown",
#             'task_confidence': 0.0,
#             'round1_cocktails': [],
#             'round1_context': "",
#             'round1_answer': "",
#             'final_cocktails': [],
#             'final_context': "",
#             'final_answer': "",
#             'iteration_count': 0,
#             'final_round': 0,
#             # Round1 scores (ê¸°ë³¸ê°’)
#             'round1_relevance': 0,
#             'round1_diversity': 0,
#             'round1_completeness': 0,
#             'round1_coherence': 0,
#             'round1_overall_score': 0,
#             'round1_should_retry': False,
#             # Final scores (ê¸°ë³¸ê°’)
#             'final_relevance': 0,
#             'final_diversity': 0,
#             'final_completeness': 0,
#             'final_coherence': 0,
#             'final_overall_score': 0,
#             'final_should_retry': False
#         }
#         return error_result

# def run_evaluation(test_case):
#     """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í‰ê°€ ì‹¤í–‰ (Ablation: Simple RAG)"""
#     print(f"\nğŸ” í‰ê°€ ì‹œì‘: Index {test_case['Index']}")
    
#     try:
#         # ì´ë¯¸ì§€ ê²½ë¡œ êµ¬ì„±
#         image_path = os.path.join(question_image_path, f"{test_case['fileNumber']}.jpeg")
#         print(f"ğŸ“¸ ì´ë¯¸ì§€: {image_path}")
        
#         # ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
#         print("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘...")
#         if os.path.exists(image_path):
#             image_description = describe_image(image_path)
#             print(f"âœ… ì´ë¯¸ì§€ ì„¤ëª…: {image_description}")
#         else:
#             print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {image_path}")
#             image_description = f"[ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {image_path}]"
        
#         # ì‚¬ìš©ì ì§ˆë¬¸ ì¡°í•©
#         user_question = f"{image_description} {test_case['query_EN']}"
#         print(f"â“ ì‚¬ìš©ì ì§ˆë¬¸: {user_question}")
        
#         # ë‹¨ìˆœ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
#         print("ğŸš€ Simple RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...")
#         result = run_simple_rag_pipeline(user_question)
        
#         # í‰ê°€ ë°ì´í„° ì¶”ì¶œ
#         print("ğŸ“Š í‰ê°€ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
#         pipeline_data = extract_pipeline_data(result)
        
#         # ê¸°ë³¸ ì •ë³´ì™€ ê²°í•©
#         evaluation_result = {
#             # ê¸°ì¡´ ì»¬ëŸ¼ë“¤
#             'Task': test_case['Task'],
#             'Index': test_case['Index'],
#             'image': test_case['image'],
#             'query_KO': test_case['query_KO'],
#             'query_EN': test_case['query_EN'],
#             'fileNumber': test_case['fileNumber'],
#             'imageCocktailName': test_case['imageCocktailName'],
            
#             # RAGAS í‰ê°€ìš© í•µì‹¬ ë°ì´í„°
#             'question': pipeline_data['user_question'],
#             'task_type': pipeline_data['task_type'],
#             'task_confidence': pipeline_data['task_confidence'],
#             'contexts': json.dumps([pipeline_data['final_context']], ensure_ascii=False),
#             'answer': pipeline_data['final_answer'],
            
#             # ì¶”ê°€ ìƒì„¸ ë°ì´í„°
#             'round1_cocktails': json.dumps(pipeline_data['round1_cocktails'], ensure_ascii=False),
#             'round1_context': pipeline_data['round1_context'],
#             'round1_answer': pipeline_data['round1_answer'],
#             'final_cocktails': json.dumps(pipeline_data['final_cocktails'], ensure_ascii=False),
#             'final_context': pipeline_data['final_context'],
#             'final_answer': pipeline_data['final_answer'],
#             'iteration_count': pipeline_data['iteration_count'],
#             'final_round': pipeline_data['final_round'],
            
#             # Round1 scores (ê°œë³„ ì»¬ëŸ¼)
#             'round1_relevance': pipeline_data['round1_relevance'],
#             'round1_diversity': pipeline_data['round1_diversity'],
#             'round1_completeness': pipeline_data['round1_completeness'],
#             'round1_coherence': pipeline_data['round1_coherence'],
#             'round1_overall_score': pipeline_data['round1_overall_score'],
#             'round1_should_retry': pipeline_data['round1_should_retry'],
            
#             # Final scores (ê°œë³„ ì»¬ëŸ¼)
#             'final_relevance': pipeline_data['final_relevance'],
#             'final_diversity': pipeline_data['final_diversity'],
#             'final_completeness': pipeline_data['final_completeness'],
#             'final_coherence': pipeline_data['final_coherence'],
#             'final_overall_score': pipeline_data['final_overall_score'],
#             'final_should_retry': pipeline_data['final_should_retry'],
            
#             # Ablation ì •ë³´
#             'method': 'simple_rag'
#         }
        
#         print(f"âœ… í‰ê°€ ì™„ë£Œ: Index {test_case['Index']}")
#         return evaluation_result
        
#     except Exception as e:
#         print(f"âŒ í‰ê°€ ì˜¤ë¥˜ (Index {test_case['Index']}): {e}")
#         import traceback
#         traceback.print_exc()
#         return None

# def main():
#     """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (Ablation: Simple RAG)"""
#     start_time = time.time()
    
#     print(f"ğŸš€ {config.LLM_MODEL} ëª¨ë¸ Ablation í‰ê°€ ì‹œì‘ (Simple RAG)")
#     print(f"ğŸ“‹ ì‚¬ìš© ëª¨ë¸: {config.LLM_MODEL}")
#     print(f"ğŸ§ª Ablation ì¡°ê±´: task ë¶„ë¥˜ + reflection + ë‹¨ìˆœ ìœ ì‚¬ë„ ê²€ìƒ‰")
    
#     # í‰ê°€ ë°ì´í„° ë¡œë“œ
#     print("\nğŸ“‚ í‰ê°€ ë°ì´í„° ë¡œë“œ ì¤‘...")
#     eval_data = load_eval_data()
#     print(f"âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜: {len(eval_data)}")
    
#     # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
#     os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
#     # ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰ (tqdm ì§„í–‰ë¥  í‘œì‹œ)
#     results = []
#     for _, test_case in tqdm(eval_data.iterrows(), total=len(eval_data), desc="Simple RAG í‰ê°€ìš© ë‹µë³€ ìƒì„±"):
#         result = run_evaluation(test_case)
#         if result:
#             results.append(result)
    
#     # ê²°ê³¼ ì €ì¥
#     if results:
#         print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘... ({len(results)}ê°œ ì¼€ì´ìŠ¤)")
        
#         df_results = pd.DataFrame(results)
#         df_results.to_csv(output_file, index=False, encoding='utf-8-sig')
        
#         print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_file}")
#         print(f"ğŸ“Š ì»¬ëŸ¼ ìˆ˜: {len(df_results.columns)}")
#         print(f"ğŸ“ˆ ë°ì´í„° í–‰ ìˆ˜: {len(df_results)}")
        
#         # ì»¬ëŸ¼ ì •ë³´ ì¶œë ¥
#         print("\nğŸ“‹ ìƒì„±ëœ ì»¬ëŸ¼ë“¤:")
#         for i, col in enumerate(df_results.columns, 1):
#             print(f"  {i:2d}. {col}")
#     else:
#         print("âŒ ìƒì„±ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
#     # ì „ì²´ ì‹¤í–‰ì‹œê°„ ì¶œë ¥
#     end_time = time.time()
#     total_time = end_time - start_time
#     minutes = int(total_time // 60)
#     seconds = total_time % 60
    
#     print(f"\nâ±ï¸ ì „ì²´ ì‹¤í–‰ì‹œê°„: {minutes}ë¶„ {seconds:.1f}ì´ˆ ({total_time:.1f}ì´ˆ)")
#     if len(results) > 0:
#         avg_time = total_time / len(results)
#         print(f"ğŸ“ˆ í‰ê·  ì¼€ì´ìŠ¤ë‹¹ ì²˜ë¦¬ì‹œê°„: {avg_time:.1f}ì´ˆ")

# if __name__ == "__main__":
#     main()